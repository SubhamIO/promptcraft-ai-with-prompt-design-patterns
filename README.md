# promptcraft-ai-with-prompt-design-patterns
Streamlit web application for generating and improving AI prompts using various prompt design patterns.

Here's a complete **`README.md`** file for your project â€” written from the perspective of a **professional content writer** and **Generative AI developer**.

---

## ğŸ§  PromptCraft AI â€” Intelligent Prompt Generator & Improver - powerd by Prompt Design Patterns

> A multi-node LangGraph-based agent pipeline that **generates, evaluates, critiques**, and **improves LLM prompts** dynamically using Groq's blazing-fast **LLaMA 3.1 8B** model.
> Built for developers, researchers, and prompt engineers who demand **high-quality prompts**.

---
---

## ğŸš€ Features

- ğŸ¯ **Prompt Generation** from task descriptions
- ğŸ”§ **Prompt Improvement** with context
- ğŸ¨ Choose from different **Prompt Design Patterns**:
  - Persona Pattern
  - Flipped Interaction
  - N-Shot Prompting
  - Directional Stimulus
  - Template Pattern
  - Meta Language Pattern
- ğŸ“œ View the **base template** used behind the scenes
- ğŸ” Uses `.env` file for secure API key management

---

### ğŸ“¸ Project Architecture

```mermaid
flowchart TD
    A[Dispatcher Node] -->|mode == generate| B[ContextBuilder]
    A -->|mode == improve| I[PromptImproverDirect]

    B --> C[PromptTemplateSelector]
    C --> D[PromptGenerator]
    D --> E[PromptEvaluator]

    E -->|score < 0.7| F[CritiqueNode]
    E -->|score >= 0.7| Z[END]

    F --> G[LoopImproverAgent]
    G --> E

    I --> Z[END]
```

---

## âœ¨ Features

* ğŸ” **Graph-based modular pipeline** using LangGraph
* ğŸš€ Powered by **Groq's LLaMA 3.1 8B-Instant** (ultra-low latency)
* ğŸ§  Supports both:

  * `generate`: full prompt generation with iterative refinement
  * `improve`: direct enhancement of existing prompts using context
* âœ… Automatic prompt **evaluation, scoring, and feedback loop**
* ğŸ› ï¸ Easily extensible (e.g., add scoring history, memory, fine-tuned prompt templates)

---

## ğŸ“Œ Node Descriptions

- **Dispatcher**  
  Entry point of the LangGraph. It routes the input based on the selected `mode`:  
  - `"generate"` â†’ goes through the full prompt creation and evaluation loop  
  - `"improve"` â†’ directly improves the given prompt using context

- **ContextBuilder**  
  Adds default context to the task description to provide grounding for prompt generation.

- **PromptTemplateSelector**  
  Applies the selected **prompt design pattern** (e.g., persona, flipped, n-shot) to create a base template prompt.

- **PromptGenerator**  
  Takes the generated base template and invokes the LLM to produce a first draft of the prompt.

- **PromptEvaluator**  
  Evaluates the quality of the generated prompt using a scoring mechanism (0.0 to 1.0).  
  If the score is below 0.7, the graph continues into critique mode.

- **CritiqueNode**  
  Provides critical feedback on the prompt and suggests how it could be improved.

- **LoopImproverAgent**  
  Uses the feedback from `CritiqueNode` to refine the prompt. The new prompt is re-evaluated in a loop until it meets quality standards.

- **PromptImproverDirect**  
  Used only in `"improve"` mode. Enhances the user's existing prompt based on additional context without scoring or iteration.


## ğŸ“¦ Tech Stack

| Tool                 | Role                           |
| -------------------- | ------------------------------ |
| **LangGraph**        | Multi-node graph orchestration |
| **LangChain**        | Prompt templates, chaining     |
| **Groq + LLaMA 3.1** | Lightning-fast inference       |
| **Python**           | Core logic & orchestration     |
| **Dotenv**           | Secure API key management      |

---

## ğŸ§‘â€ğŸ’» How It Works

### ğŸ”¹ 1. Prompt Generation Mode (`mode: generate`)

* Builds context based on task
* Creates a base template based on the prompt design pattern selected
* Generates prompt using LLM
* Evaluates and scores prompt quality
* If score < 0.7, it:

  * Critiques the prompt
  * Refines it using feedback
  * Re-evaluates until threshold met

### ğŸ”¹ 2. Prompt Improvement Mode (`mode: improve`)

* Accepts user-defined prompt + improvement context
* Directly sends to an LLM for enhancement

---

## ğŸ›  Installation

1. **Clone the Repo**

```bash
git clone https://github.com/SubhamIO/promptcraft-ai-with-prompt-design-patterns.git
cd promptcraft-ai-with-prompt-design-patterns
```

2. **Install Dependencies**

```bash
pip install -r requirements.txt
```

3. **Add Environment Variables**

Create a `.env` file with your Groq API key:

```
GROQ_API_KEY=your-groq-api-key-here
```

---

## ğŸš€ Usage

Run the script:

```bash
app_promptpatterns_pythonapp.py or streamlit run app_promptpatterns.py
```

Expected output:

```bash
Running: Full Prompt Generation and Iterative Improvement
--- Evaluator: Score = 0.55, Issue Found = True ---
--- Critique Generated ---
--- Prompt Improved in Loop ---
--- Final Generated Prompt ---
"Explain the theory of relativity to a 10-year-old using a story about two children playing catch on a moving train..."

...

Running: Direct Prompt Improvement with Context
--- Final Improved Prompt ---
"Explain relativity using a train and ball analogy that a child can understand."
```

---

## ğŸ“‚ Directory Structure

```
ğŸ“¦ promptcraft-ai/
 â”£ ğŸ“„ app_promptpatterns.py
 â”£ ğŸ“„ pipeline_promptpatterns.py
 â”£ ğŸ“„ README.md
 â”£ ğŸ“„ requirements.txt
 â”£ ğŸ“„ .streamlit/secrets.toml
```

---

---

## ğŸŒ± Future Enhancements

* ğŸ§  Add memory-aware prompt history using `LangChain Memory`
* ğŸ“Š Dashboard for prompt score tracking
* ğŸ”— API endpoints to use in other apps
* ğŸ§ª Custom fine-tuning of evaluation thresholds

---

## ğŸ™Œ Contributing

Feel free to fork, suggest improvements, or raise issues.
This project is designed to be a plug-and-play module for **any LLM-powered workflow.**

---

## ğŸ“„ License

MIT License Â© 2025 \[Subham Sarkar]


